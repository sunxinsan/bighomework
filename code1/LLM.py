# camouflage_attack_svm_guided_final.py
# æœ€ç»ˆç‰ˆï¼šSVM æŒ‡å¯¼æ”¹å†™ â€”â€” æå–é«˜å±å…³é”®è¯ï¼Œè®© GLM é‡ç‚¹æ›¿æ¢

import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import time
import pandas as pd
import joblib
from zhipuai import ZhipuAI
import jieba
import torch
from bert_score import score as bertscore
import numpy as np
import re
from scipy.sparse import issparse

# å¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼
def chinese_tokenizer(text):
    return jieba.lcut(text)

# ==============================
# ğŸ”§ é…ç½®åŒºï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
# ==============================
CONFIG = {
    "input_csv": "data/æµ‹è¯•é›†ç»“æœ.csv",
    "output_csv": "results/camouflage_svm_guided_final.csv",
    "svm_model": "svm_fraud_detector_left_only.pkl",
    "vectorizer": "tfidf_vectorizer_left_only.pkl",
    "api_key": "df9feb23f1a649d585b804dce3eeb7d6.ExubWLd71EDIW2tk",  # æ›¿æ¢ä¸ºä½ çš„ GLM API Key
    "max_samples": 10,
    "max_iterations": 4,
    "min_similarity": 0.70,
    "use_cuda": False,
    "top_k_keywords": 8  # SVM æŒ‡å¯¼æ—¶æå–çš„é«˜å±è¯æ•°é‡
}

def compute_bertscore(original: str, rewritten: str, use_cuda: bool = False) -> float:
    """ä½¿ç”¨ä¸­æ–‡ä¸“ç”¨ BERT è®¡ç®—æ›´å‡†ç¡®çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
    try:
        P, R, F1 = bertscore(
            [rewritten],
            [original],
            lang="zh",
            model_type="bert-base-chinese",
            rescale_with_baseline=False,
            device="cuda" if use_cuda and torch.cuda.is_available() else "cpu"
        )
        return float(F1.mean().item())
    except Exception as e:
        print(f"âš ï¸ BERTScore è®¡ç®—å¤±è´¥ï¼Œè¿”å› 0.0: {e}")
        return 0.0

def extract_high_risk_keywords(text, vectorizer, svm_model, top_k=10):
    """
    ä»æ–‡æœ¬ä¸­æå–é«˜é£é™©å…³é”®è¯
    """
    # 1. æ–‡æœ¬é¢„å¤„ç† (ç¡®ä¿ä¸è®­ç»ƒæ—¶ä¸€è‡´)
    clean_text = re.sub(r'^éŸ³é¢‘å†…å®¹ï¼š\s*', '', str(text))
    clean_text = re.sub(r'(å®¢æœ|ç”¨æˆ·|left|right):', '', clean_text, flags=re.IGNORECASE)
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    
    # åˆ†è¯
    processed_text = ' '.join([w for w in jieba.lcut(clean_text) if len(w) > 1])
    
    # 2. å‘é‡åŒ–
    try:
        tfidf_vec = vectorizer.transform([processed_text])
    except Exception as e:
        print(f"å‘é‡åŒ–å¤±è´¥: {e}")
        return ["è½¬è´¦", "éªŒè¯ç ", "å…¬å®‰å±€", "æ¶‰å«Œ", "å†»ç»“", "å®‰å…¨è´¦æˆ·"]  # é»˜è®¤æ•æ„Ÿè¯
    
    # 3. è·å–éé›¶ç‰¹å¾ç´¢å¼•
    rows, cols = tfidf_vec.nonzero()
    
    # 4. å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
    valid_indices = []
    for idx in cols:
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦åœ¨ [0, vocab_size-1] èŒƒå›´å†…
        if idx < len(vectorizer.get_feature_names_out()):
            valid_indices.append(idx)
        else:
            print(f"âš ï¸  è­¦å‘Š: ç´¢å¼• {idx} è¶…å‡ºäº†è¯æ±‡è¡¨èŒƒå›´ (max_index={len(vectorizer.get_feature_names_out())-1})")
    
    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ç´¢å¼•ï¼Œè¿”å›é»˜è®¤æ•æ„Ÿè¯
    if not valid_indices:
        return ["è½¬è´¦", "éªŒè¯ç ", "å…¬å®‰å±€", "æ¶‰å«Œ", "å†»ç»“", "å®‰å…¨è´¦æˆ·"]
    
    # 5. è®¡ç®—æ¯ä¸ªç‰¹å¾çš„è´¡çŒ®åº¦ (TF-IDF * SVM æƒé‡)
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ svm_model.coef_ æ˜¯å¯†é›†æ•°ç»„æˆ–å¯ä»¥è¢«ç´¢å¼•
    # å¦‚æœæ˜¯ç¨€ç–çŸ©é˜µï¼Œéœ€è¦è½¬æ¢æˆ–ä½¿ç”¨ .toarray()
    if issparse(svm_model.coef_):
        coef_array = svm_model.coef_.toarray()[0]  # è½¬æ¢ä¸ºå¯†é›†æ•°ç»„
    else:
        coef_array = svm_model.coef_[0]
    
    contributions = []
    feature_names = vectorizer.get_feature_names_out()
    
    for idx in valid_indices:
        word = feature_names[idx]
        weight = coef_array[idx]
        contribution = tfidf_vec[0, idx] * weight
        contributions.append((word, contribution))
    
    # 6. æ’åºå¹¶è¿”å›å‰ top_k ä¸ª
    sorted_contributions = sorted(contributions, key=lambda x: abs(x[1]), reverse=True)
    high_risk_words = [word for word, _ in sorted_contributions[:top_k]]
    
    return high_risk_words

def build_attacker_prompt(original: str, history: list, high_risk_words: list) -> str:
    """æ„å»ºåŒ…å« SVM é«˜å±è¯æŒ‡å¯¼çš„ Prompt"""
    
    SPEAKER_PREFIXES = ["left:", "right:", "å®¢æœ:", "ç”¨æˆ·:", "A:", "B:", "[User]:", "Agent:"]
    has_speaker = any(original.strip().startswith(p) for p in SPEAKER_PREFIXES)
    
    if has_speaker:
        for prefix in SPEAKER_PREFIXES:
            if original.startswith(prefix):
                speaker = prefix
                content = original[len(prefix):].strip()
                break
        instruction_prefix = f"ã€åŸå§‹å¥å­ã€‘\n{original}\n\nã€æ”¹å†™è¦æ±‚ã€‘\nä»…æ”¹å†™å†’å·åçš„å†…å®¹ï¼Œå¿…é¡»ä¿ç•™ '{speaker}' å‰ç¼€ä¸å˜ã€‚"
    else:
        speaker = None
        instruction_prefix = f"ã€åŸå§‹å¥å­ã€‘\n{original}"

    # é»˜è®¤åŒä¹‰æ›¿æ¢åº“ï¼ˆç”¨äº fallback å’Œç¤ºä¾‹ï¼‰
    default_synonyms = {
        "è½¬è´¦": ["æ‰“æ¬¾", "æ±‡æ¬¾", "æŠŠé’±è½¬è¿‡å»", "èµ„é‡‘åˆ’è½¬"],
        "éªŒè¯ç ": ["çŸ­ä¿¡å¯†ç ", "åŠ¨æ€ç ", "æ‰‹æœºæ”¶åˆ°çš„6ä½æ•°", "ä¸€æ¬¡æ€§å£ä»¤"],
        "å…¬å®‰å±€": ["å…¬å®‰éƒ¨é—¨", "è­¦æ–¹", "å½“åœ°æ´¾å‡ºæ‰€", "å…¬å®‰æœºå…³"],
        "æ¶‰å«Œ": ["å¯èƒ½æ¶‰åŠ", "æœ‰å«Œç–‘", "è¢«æŒ‡ç§°"],
        "ç«‹å³": ["å°½å¿«", "é©¬ä¸Š", "æŠ“ç´§æ—¶é—´"],
        "å®‰å…¨è´¦æˆ·": ["ä¿æŠ¤è´¦æˆ·", "ä¸“ç”¨è´¦å·", "æ ¸æŸ¥ç”¨çš„è´¦å·", "ä¸´æ—¶ä¿ç®¡è´¦æˆ·"],
        "å†»ç»“": ["æš‚æ—¶é”ä½", "æš‚åœä½¿ç”¨", "é™åˆ¶æ“ä½œ"],
        "ç‚¹å‡»é“¾æ¥": ["ç‚¹å¼€é‚£ä¸ªç½‘å€", "è®¿é—®ä¸€ä¸‹é¡µé¢", "æ‰“å¼€æä¾›çš„åœ°å€"],
        "é…åˆè°ƒæŸ¥": ["ååŠ©æ ¸å®", "å¸®å¿™ç¡®è®¤æƒ…å†µ", "æŒ‰æµç¨‹èµ°ä¸€ä¸‹"]
    }

    # åŠ¨æ€ç”Ÿæˆé«˜å±è¯æ›¿æ¢å»ºè®®
    dynamic_examples = "ã€æœ¬æ¬¡éœ€é‡ç‚¹æ›¿æ¢çš„é«˜å±è¯ï¼ˆæ¥è‡ªSVMåˆ†æï¼‰ã€‘\n"
    for word in high_risk_words:
        candidates = default_synonyms.get(word, [f"åŒä¹‰è¡¨è¾¾_{word}"])
        alt = "ã€".join(candidates[:3])
        dynamic_examples += f"- â€œ{word}â€ â†’ {alt}\n"

    base_instruction = f"""ä½ æ˜¯ä¸€ä½ç²¾é€šä¸­æ–‡å¯¹æŠ—æ€§æ–‡æœ¬ç”Ÿæˆçš„ä¸“å®¶ã€‚ä»»åŠ¡æ˜¯å¯¹ä»¥ä¸‹å¥å­è¿›è¡Œ**åŒä¹‰æ”¹å†™**ï¼Œç›®æ ‡æ˜¯ç»•è¿‡è¯ˆéª—æ£€æµ‹ç³»ç»Ÿã€‚

ã€æ ¸å¿ƒåŸåˆ™ã€‘
1. âœ… **å¿…é¡»ä¼˜å…ˆå¯¹ä¸Šè¿°é«˜å±è¯è¿›è¡Œæ›¿æ¢**ï¼è¿™æ˜¯ç»•è¿‡æ£€æµ‹çš„å…³é”®ã€‚
2. ğŸ”’ **é‡‘é¢ã€é“¶è¡Œå¡å·ã€æ‰‹æœºå·ã€èº«ä»½è¯å·ã€å…·ä½“æ—¶é—´ç­‰å…³é”®ä¿¡æ¯å¿…é¡»åŸæ ·ä¿ç•™**ã€‚
3. ğŸ—£ï¸ **å¦‚æœåŸæ–‡æœ‰ 'left:'ã€'right:' ç­‰å‰ç¼€ï¼Œå¿…é¡»åŸæ ·ä¿ç•™ï¼Œä»…æ”¹å†™å…¶åå†…å®¹**ã€‚
4. åªæ”¹å†™ä»¥leftä¸ºå‰ç¼€çš„å¥å­å†…å®¹ï¼ˆå¦‚æœæœ‰å‰ç¼€çš„è¯ï¼‰ã€‚
5. åœ¨ä¿æŒå¥å­å¤§æ¦‚åŸæ„çš„æƒ…å†µä¸‹ï¼Œæ”¹å†™æ•´ä¸ªå¥å­ï¼Œé¿å…ä»…æ›¿æ¢å•è¯ã€‚

{dynamic_examples}

âš ï¸ ç¦æ­¢è¡Œä¸ºï¼š
- åˆ é™¤ã€ä¿®æ”¹æˆ–çœç•¥è¯´è¯äººæ ‡è¯†ï¼ˆå¦‚æŠŠ 'left:' åˆ æ‰ âŒï¼‰ï¼›
- æ·»åŠ è§£é‡Šæ€§å‰ç¼€ï¼ˆå¦‚â€œæ”¹å†™ç»“æœï¼šâ€ï¼‰ï¼›
- æ”¹å˜äº‹å®ï¼ˆå¦‚â€œè½¬è´¦â€â†’â€œæ”¶æ¬¾â€ï¼‰ã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘
- ä»…è¾“å‡ºä¸€è¡Œæ”¹å†™åçš„å®Œæ•´å¥å­ï¼›
- æ ¼å¼å¿…é¡»ä¸åŸæ–‡å®Œå…¨ä¸€è‡´ï¼ˆå¦‚æœ‰å‰ç¼€ï¼Œåˆ™å¿…é¡»ä¿ç•™ï¼‰ã€‚

{instruction_prefix}

ã€æ”¹å†™ç»“æœã€‘
"""

    if not history:
        return base_instruction

    # === åŠ¨æ€åé¦ˆæœºåˆ¶ ===
    successful = [h for h in history if h["svm_pred"] == 0]
    last = history[-1]

    feedback = "\nã€å†å²åé¦ˆä¸ç­–ç•¥è°ƒæ•´ã€‘\n"

    if successful:
        ex = successful[-1]["text"]
        feedback += f"- âœ… æˆåŠŸæ¡ˆä¾‹ï¼š\"{ex[:70]}...\"\n"
        feedback += "- è¯·ä¿æŒç›¸åŒæ ¼å¼ï¼ˆåŒ…æ‹¬å¯¹è¯æ–¹å‰ç¼€ï¼‰ã€‚\n"
    else:
        if last["similarity"] < CONFIG["min_similarity"]:
            feedback += "- âŒ è¯­ä¹‰åç¦»è¿‡å¤§ â†’ è¯·ç¡®ä¿é‡‘é¢ã€è´¦å·ã€è¯´è¯äººå‰ç¼€å‡ä¿ç•™ã€‚\n"
        else:
            feedback += "- âš ï¸ è¯­ä¹‰è¶³å¤Ÿä½†æœªéª—è¿‡æ¨¡å‹ â†’ è¯·æ›´ç§¯ææ›¿æ¢é«˜å±è¯ï¼\n"
            feedback += "  åŒæ—¶æ³¨æ„ï¼š**ä¸è¦åˆ æ‰ left/right ç­‰å¯¹è¯æ–¹æ ‡è¯†ï¼**\n"
    
    feedback += "- ğŸ”‘ è®°ä½ï¼šè¯´è¯äººæ ‡è¯†æ˜¯æ ¼å¼çš„ä¸€éƒ¨åˆ†ï¼Œå¿…é¡»åŸæ ·ä¿ç•™ï¼"

    return base_instruction + feedback

def rewrite_with_glm(client, prompt: str) -> str:
    """è°ƒç”¨ GLMï¼Œé¿å…è¯¯åˆ  left:/right: ç­‰åˆæ³•å‰ç¼€"""
    for _ in range(2):
        try:
            resp = client.chat.completions.create(
                model="glm-4-flash",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=256
            )
            result = resp.choices[0].message.content.strip()
            
            # æ¸…ç† LLM å¯èƒ½æ·»åŠ çš„è¯´æ˜æ€§å‰ç¼€ï¼Œä½†ä¿ç•™ left:/right:
            unwanted_prefixes = [
                "æ”¹å†™ç»“æœï¼š", "è¾“å‡ºï¼š", "ï¼š", "å¥å­ï¼š", "â€œ", "â€", 
                "æ”¹å†™ï¼š", "ã€æ”¹å†™ç»“æœã€‘", "ç»“æœï¼š", "ç­”ï¼š", "æ”¹å†™åçš„å¥å­ï¼š"
            ]
            for p in unwanted_prefixes:
                if result.startswith(p):
                    result = result[len(p):].strip()
            
            return result if result else ""
        except Exception as e:
            print(f"  âš ï¸ GLM è°ƒç”¨å‡ºé”™: {e}ï¼Œé‡è¯•...")
            time.sleep(1)
    return ""

def predict_with_svm(text: str, svm_model, vectorizer) -> int:
    try:
        # ä¸æå–å…³é”®è¯æ—¶çš„æ¸…æ´—é€»è¾‘ä¿æŒä¸€è‡´
        clean_text = re.sub(r'^éŸ³é¢‘å†…å®¹ï¼š\s*', '', str(text))
        clean_text = re.sub(r'(å®¢æœ|ç”¨æˆ·|left|right):', '', clean_text, flags=re.IGNORECASE)
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        processed_text = ' '.join([w for w in jieba.lcut(clean_text) if len(w) > 1])
        
        vec = vectorizer.transform([processed_text])
        pred = svm_model.predict(vec)[0]
        return int(pred)
    except Exception as e:
        print(f"  âš ï¸ SVM é¢„æµ‹å¼‚å¸¸: {e}")
        return 1

def attack_single_sample(
    client,
    original: str,
    svm_model,
    vectorizer,
    max_iters: int,
    min_sim: float,
    use_cuda: bool,
    top_k: int
) -> dict:
    # === æå–é«˜å±å…³é”®è¯ï¼ˆåªåšä¸€æ¬¡ï¼‰===
    high_risk_words = extract_high_risk_keywords(original, vectorizer, svm_model, top_k=top_k)
    print(f"  ğŸ” SVM é«˜å±è¯: {high_risk_words}")

    history = []
    best_result = original
    best_sim = 0.0
    final_pred = 1
    success = False
    used_iters = 0

    print(f"  ğŸ“ åŸå§‹æ–‡æœ¬: {original}")

    for it in range(1, max_iters + 1):
        used_iters = it
        
        attacker_prompt = build_attacker_prompt(original, history, high_risk_words)
        rewritten = rewrite_with_glm(client, attacker_prompt)
        if not rewritten.strip():
            rewritten = original

        sim_score = compute_bertscore(original, rewritten, use_cuda)
        svm_pred = predict_with_svm(rewritten, svm_model, vectorizer)
        
        history.append({
            "text": rewritten,
            "similarity": sim_score,
            "svm_pred": svm_pred
        })

        if sim_score > best_sim:
            best_sim = sim_score
            best_result = rewritten
            final_pred = svm_pred

        status = "âœ… æˆåŠŸ" if (svm_pred == 0 and sim_score >= min_sim) else "âŒ å¤±è´¥"
        print(f"    â†’ ç¬¬ {it} è½®: [SVM={svm_pred}] [BERTScore={sim_score:.3f}] {status}")
        print(f"      æ”¹å†™: {rewritten[:150]}{'...' if len(rewritten) > 150 else ''}")

        if svm_pred == 0 and sim_score >= min_sim:
            success = True
            break

        time.sleep(0.3)

    return {
        "adversarial": best_result,
        "attack_success": success,
        "bertscore_similarity": best_sim,
        "svm_prediction_after": final_pred,
        "final_iteration": used_iters,
        "total_attempts": len(history),
        "high_risk_words": ",".join(high_risk_words)  # ä¾¿äºä¿å­˜åˆ†æ
    }

def main():
    args = CONFIG
    
    os.makedirs(os.path.dirname(args["output_csv"]), exist_ok=True)
    client = ZhipuAI(api_key=args["api_key"])
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv(args["input_csv"])
    if "specific_dialogue_content" not in df.columns:
        raise ValueError("è¾“å…¥ CSV å¿…é¡»åŒ…å« 'specific_dialogue_content' åˆ—")

    # åŠ è½½æ¨¡å‹
    svm_model = joblib.load(args["svm_model"])
    vectorizer = joblib.load(args["vectorizer"])

    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¿æ€§ SVM
    if not hasattr(svm_model, 'coef_') or svm_model.coef_.ndim != 2:
        raise ValueError("âŒ ä»…æ”¯æŒçº¿æ€§ SVMï¼ˆå¦‚ LinearSVC æˆ– SVC(kernel='linear')ï¼‰")

    # === ç”¨ SVM é¢„æµ‹æ‰€æœ‰æ ·æœ¬ï¼Œåªæ”»å‡»é¢„æµ‹ä¸º 1 çš„ ===
    print("ğŸ” æ­£åœ¨ç”¨ SVM é¢„æµ‹æ‰€æœ‰æ ·æœ¬ï¼Œç­›é€‰å¯æ”»å‡»å¯¹è±¡ï¼ˆSVM_pred == 1ï¼‰...")
    svm_preds_all = []
    for idx, row in df.iterrows():
        text = str(row["specific_dialogue_content"]).strip()
        pred = predict_with_svm(text, svm_model, vectorizer) if text else 0
        svm_preds_all.append(pred)
    
    df["svm_prediction_original"] = svm_preds_all
    attackable_mask = df["svm_prediction_original"] == 1
    attackable_indices = df[attackable_mask].index.tolist()[:args["max_samples"]]
    print(f"ğŸ¯ å…±æ‰¾åˆ° {len(df[attackable_mask])} ä¸ª SVM æˆåŠŸæ£€å‡ºçš„è¯ˆéª—æ ·æœ¬ï¼Œå°†æ”»å‡»å‰ {len(attackable_indices)} ä¸ª\n")

    # åˆå§‹åŒ–è¾“å‡º DataFrame
    output_df = df.copy()
    extra_cols = [
        "adversarial", "attack_success", "bertscore_similarity",
        "svm_prediction_after", "final_iteration", "high_risk_words"
    ]
    for col in extra_cols:
        output_df[col] = None

    success_count = 0
    for i, idx in enumerate(attackable_indices, 1):
        original = str(df.at[idx, "specific_dialogue_content"]).strip()
        if not original:
            continue

        print(f"[{i}/{len(attackable_indices)}] åŸæ–‡: {original[:60]}{'...' if len(original) > 60 else ''}")
        result = attack_single_sample(
            client=client,
            original=original,
            svm_model=svm_model,
            vectorizer=vectorizer,
            max_iters=args["max_iterations"],
            min_sim=args["min_similarity"],
            use_cuda=args["use_cuda"],
            top_k=args["top_k_keywords"]
        )

        for k, v in result.items():
            output_df.at[idx, k] = v

        if result["attack_success"]:
            success_count += 1

    # æ‰€æœ‰æœªè¢«æ”»å‡»çš„æ ·æœ¬ç›´æ¥å¤åˆ¶åŸæ–‡
    not_attacked_mask = ~df.index.isin(attackable_indices)
    output_df.loc[not_attacked_mask, "adversarial"] = df.loc[not_attacked_mask, "specific_dialogue_content"]
    output_df.loc[not_attacked_mask, "attack_success"] = False
    output_df.loc[not_attacked_mask, "bertscore_similarity"] = 1.0
    output_df.loc[not_attacked_mask, "svm_prediction_after"] = df.loc[not_attacked_mask, "svm_prediction_original"]
    output_df.loc[not_attacked_mask, "final_iteration"] = 0
    output_df.loc[not_attacked_mask, "high_risk_words"] = ""

    # ä¿å­˜ç»“æœ
    output_df.to_csv(args["output_csv"], index=False, encoding="utf-8-sig")
    
    print("\n" + "="*70)
    print("âœ… CAMOUFLAGE å¯¹æŠ—æ”»å‡»å®Œæˆï¼")
    print(f"æ”»å‡»æˆåŠŸç‡: {success_count}/{len(attackable_indices)} ({100 * success_count / max(1, len(attackable_indices)):.1f}%)")
    print(f"ç»“æœå·²ä¿å­˜è‡³: {os.path.abspath(args['output_csv'])}")
    print("ğŸ’¡ æç¤ºï¼š'high_risk_words' åˆ—è®°å½•äº†æ¯æ¬¡æ”»å‡»ä½¿ç”¨çš„ SVM é«˜å±è¯ï¼Œå¯ç”¨äºåˆ†æã€‚")

if __name__ == "__main__":
    main()
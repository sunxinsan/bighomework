# evaluate_model.py
import pandas as pd
import jieba
import re
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import joblib
import os
import sys
from datetime import datetime

# ======================
# é…ç½®è·¯å¾„
# ======================
MODEL_PATH = "svm_fraud_detector_left_only.pkl"
VECTORIZER_PATH = "tfidf_vectorizer_left_only.pkl"
LOG_DIR = "logs"
RESULT_DIR = "results"

# ======================
# å¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼å¦åˆ™æ— æ³•åŠ è½½å‘é‡åŒ–å™¨
# ======================
def chinese_tokenizer(text):
    return jieba.lcut(str(text))

# ======================
# æ—¥å¿—å·¥å…·ï¼šåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
# ======================
class Logger:
    def __init__(self, log_file):
        self.terminal = sys.stdout
        self.log = open(log_file, "a", encoding="utf-8")

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        self.terminal.flush()
        self.log.flush()

# ======================
# åŠ è½½æ¨¡å‹å’Œå‘é‡åŒ–å™¨
# ======================
def load_model_and_vectorizer():
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {os.path.abspath(MODEL_PATH)}")
    if not os.path.exists(VECTORIZER_PATH):
        raise FileNotFoundError(f"âŒ å‘é‡åŒ–å™¨æ–‡ä»¶ä¸å­˜åœ¨: {os.path.abspath(VECTORIZER_PATH)}")

    model = joblib.load(MODEL_PATH)
    vectorizer = joblib.load(VECTORIZER_PATH)
    print("âœ… æ¨¡å‹ä¸å‘é‡åŒ–å™¨åŠ è½½æˆåŠŸï¼")
    return model, vectorizer

# ======================
# æå–çº¯å¯¹è¯å†…å®¹ï¼ˆå»é™¤ left/right æ ‡è®°ï¼‰
# ======================
def extract_dialogue(text):
    if pd.isna(text):
        return ""
    clean = str(text).replace("éŸ³é¢‘å†…å®¹ï¼š", "").rstrip(" **")
    # æå– left/right åçš„å†…å®¹
    turns = re.findall(r'(?:left|right):\s*(.*?)(?=\s*(?:left|right):|\s*$)', clean)
    return " ".join(turn.strip() for turn in turns if turn.strip())

# ======================
# åŠ è½½æµ‹è¯•é›†ï¼ˆå…³é”®ä¿®å¤ï¼šå¤„ç†éå­—ç¬¦ä¸²æ ‡ç­¾ï¼‰
# ======================
def load_test_data(csv_path, has_header=True):
    df = pd.read_csv(csv_path, header=0 if has_header else None, on_bad_lines='skip')
    
    # ç¡®ä¿åˆ—åæ­£ç¡®ï¼ˆä½ çš„çœŸå®åˆ—åï¼‰
    expected_columns = ["specific_dialogue_content", "interaction_strategy", "call_type", "is_fraud", "fraud_type"]
    if has_header:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®åˆ—
        if "specific_dialogue_content" not in df.columns:
            raise ValueError(f"âŒ æ‰¾ä¸åˆ°åˆ— 'specific_dialogue_content'ã€‚å½“å‰åˆ—: {list(df.columns)}")
        if "is_fraud" not in df.columns:
            raise ValueError(f"âŒ æ‰¾ä¸åˆ°æ ‡ç­¾åˆ— 'is_fraud'ã€‚")
    else:
        # å¦‚æœæ—  headerï¼Œå¼ºåˆ¶å‘½åï¼ˆå¤‡ç”¨ï¼‰
        if len(df.columns) >= 5:
            df.columns = expected_columns
        else:
            raise ValueError("âŒ æ•°æ®åˆ—æ•°ä¸è¶³ï¼Œæ— æ³•åŒ¹é…è®­ç»ƒæ ¼å¼ã€‚")

    # æ˜ å°„ä¸ºå†…éƒ¨ä½¿ç”¨çš„ raw_text
    df["raw_text"] = df["specific_dialogue_content"]
    
    # æå–å¹²å‡€æ–‡æœ¬
    df["text"] = df["raw_text"].apply(extract_dialogue)
    df = df[df["text"].str.len() >= 5].reset_index(drop=True)
    
    # =============== å…³é”®ä¿®å¤ï¼šå¤„ç†éå­—ç¬¦ä¸²æ ‡ç­¾ ===============
    if "is_fraud" in df.columns:
        # 1. åˆ é™¤ç¼ºå¤±æ ‡ç­¾çš„è¡Œ
        initial_count = len(df)
        df = df.dropna(subset=["is_fraud"])
        dropped_count = initial_count - len(df)
        if dropped_count > 0:
            print(f"âš ï¸ å·²åˆ é™¤ {dropped_count} æ¡ç¼ºå¤±æ ‡ç­¾çš„æ ·æœ¬")
        
        # 2. ç¡®ä¿ is_fraud æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼ˆå…³é”®ä¿®å¤ï¼ï¼‰
        df["is_fraud"] = df["is_fraud"].astype(str).str.lower()
        
        # 3. ç»Ÿä¸€å¤„ç†æ ‡ç­¾å€¼ï¼ˆå¤„ç†å„ç§è¡¨ç¤ºå½¢å¼ï¼‰
        df["is_fraud"] = df["is_fraud"].replace(["true", "1", "yes", "t", "y"], "true")
        df["is_fraud"] = df["is_fraud"].replace(["false", "0", "no", "f", "n"], "false")
        
        # 4. æ˜ å°„ä¸º 0/1
        df["label"] = df["is_fraud"].map({"true": 1, "false": 0})
        
        # 5. å¤„ç†æ— æ•ˆå€¼ï¼ˆç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯ 'true' æˆ– 'false'ï¼‰
        invalid_mask = ~df["is_fraud"].isin(["true", "false"])
        invalid_count = invalid_mask.sum()
        if invalid_count > 0:
            print(f"âš ï¸ å‘ç° {invalid_count} æ¡æ— æ•ˆæ ‡ç­¾ï¼Œå·²ç»Ÿä¸€æ›¿æ¢ä¸º 'false'")
            df.loc[invalid_mask, "label"] = 0
    else:
        raise ValueError("âŒ æ ‡ç­¾åˆ— 'is_fraud' ä¸å­˜åœ¨ï¼Œæ— æ³•è¯„ä¼°ã€‚")
    
    print(f"ğŸ“Š æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼å…± {len(df)} æ¡æœ‰æ•ˆæ ·æœ¬")
    print(f"   - æ¬ºè¯ˆæ ·æœ¬: {df['label'].sum()}")
    print(f"   - æ­£å¸¸æ ·æœ¬: {len(df) - df['label'].sum()}")
    
    return df

# ======================
# è¯„ä¼°å¹¶ä¿å­˜ç»“æœ
# ======================
def evaluate_and_save_results(test_df, model, vectorizer, result_csv_path):
    X_test = vectorizer.transform(test_df["text"])
    y_pred = model.predict(X_test)
    
    result_df = test_df.copy()
    result_df["predicted_label"] = y_pred
    result_df["predicted_class"] = result_df["predicted_label"].map({1: "æ¬ºè¯ˆ", 0: "æ­£å¸¸"})
    result_df["true_class"] = result_df["label"].map({1: "æ¬ºè¯ˆ", 0: "æ­£å¸¸"})
    result_df["correct"] = (result_df["label"] == result_df["predicted_label"])
    
    # è®¡ç®—æŒ‡æ ‡ï¼ˆç°åœ¨ y_true ä¿è¯æ—  NaNï¼‰
    y_true = test_df["label"].values
    acc = accuracy_score(y_true, y_pred)
    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')
    
    print("\n=== æ¨¡å‹è¯„ä¼°ç»“æœ ===")
    print(f"å‡†ç¡®ç‡ (Accuracy): {acc:.3f}")
    print(f"æ¬ºè¯ˆç±»ç²¾ç¡®ç‡ (Precision): {prec:.3f}")
    print(f"æ¬ºè¯ˆç±»å¬å›ç‡ (Recall): {rec:.3f}")
    print(f"æ¬ºè¯ˆç±» F1 åˆ†æ•°: {f1:.3f}")
   
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_true, y_pred, target_names=["æ­£å¸¸", "æ¬ºè¯ˆ"], digits=3))
    
    # ä¿å­˜ç»“æœï¼ˆæ”¯æŒä¸­æ–‡ Excelï¼‰
    os.makedirs(os.path.dirname(result_csv_path), exist_ok=True)
    result_df.to_csv(result_csv_path, index=False, encoding="utf-8-sig")
    print(f"\nâœ… é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {result_csv_path}")
    
    return {"acc": acc, "f1": f1}

# ======================
# ä¸»æµç¨‹
# ======================
if __name__ == "__main__":
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(LOG_DIR, exist_ok=True)
    os.makedirs(RESULT_DIR, exist_ok=True)
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(LOG_DIR, f"eval_{timestamp}.log")
    result_csv = os.path.join(RESULT_DIR, f"predictions_{timestamp}.csv")
    
    # é‡å®šå‘è¾“å‡ºåˆ°æ—¥å¿— + æ§åˆ¶å°
    sys.stdout = Logger(log_file)
    
    print("=" * 60)
    print(f"ğŸš€ å¼€å§‹è¯„ä¼°æ¨¡å‹ | æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ¨¡å‹è·¯å¾„: {os.path.abspath(MODEL_PATH)}")
    
    # ğŸ‘‡ ä¿®æ”¹è¿™é‡ŒæŒ‡å®šä½ çš„æµ‹è¯•é›†è·¯å¾„ï¼ˆç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰
    test_csv = "data/æµ‹è¯•é›†ç»“æœ.csv"
    print(f"æµ‹è¯•é›†è·¯å¾„: {os.path.abspath(test_csv)}")
    print(f"ç»“æœå°†ä¿å­˜è‡³: {result_csv}")
    print("=" * 60)
    
    # æ‰§è¡Œè¯„ä¼°
    model, vectorizer = load_model_and_vectorizer()
    test_df = load_test_data(test_csv, has_header=True)  # âœ… ä½ çš„æ•°æ®æœ‰è¡¨å¤´
    evaluate_and_save_results(test_df, model, vectorizer, result_csv)
    
    print(f"\nğŸ“„ å®Œæ•´æ—¥å¿—å·²ä¿å­˜è‡³: {log_file}")